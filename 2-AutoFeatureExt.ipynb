{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd3a024-5f12-47ce-8511-c2bfae262628",
   "metadata": {},
   "source": [
    "# Tutorial 2: Automatic Feature Extraction/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed665b16-22c8-4a4d-b4db-0837264fb837",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a99b7c-2540-4862-bcca-315eeb1c0803",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5412c-eb3a-416a-81ff-ce5f77a0df88",
   "metadata": {},
   "source": [
    "In this notebook, we will extract/engineer features using a deep learning method called autoencoder.\n",
    " An autoencoder is an artificial neural network with a symmetric structure which is trained to reconstruct its input at the final output layer. The output of the first half of the network represents an encoding of the input data. ([source](https://arxiv.org/abs/2206.06165))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8030153b-d68e-4f25-8774-4b9e4bb5bfef",
   "metadata": {},
   "source": [
    "First, we import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d5526-086d-4d0e-b546-e81af265749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting data/graphs\n",
    "import numpy as np # For handling N-DIMENSIONAL ARRAYS\n",
    "\n",
    "import tensorflow as tf #An end-to-end machine learning platform, focusing on training deep learning models\n",
    "from tensorflow.keras import layers, losses # Implementation of the Keras API, the high-level API of TensorFlow.\n",
    "from tensorflow.keras.models import Model #This displays graphs \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7b065-2cd2-4d88-8329-a7b0e9591e1a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a52a2-6542-4535-9aad-0782ed0eb4a4",
   "metadata": {},
   "source": [
    "### Reading in data\n",
    "The following code is the same as in Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ba1a6-c0cd-4bb9-ae83-74143f138397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galaxy_mnist import GalaxyMNISTHighrez\n",
    "\n",
    "dataset_train = GalaxyMNISTHighrez(\n",
    "    root='data_import/data',\n",
    "    download=True,\n",
    "    train=True  # by default, or False for canonical test set\n",
    ")\n",
    "# for the testing data\n",
    "dataset_test = GalaxyMNISTHighrez(\n",
    "    root='data_import/data',\n",
    "    download=True,\n",
    "    train=False  # by default, or False for canonical test set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4639a4d-6649-48e5-8fec-47d8026828fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the training and testing labels and image samples\n",
    "images_train = dataset_train.data\n",
    "images_test = dataset_test.data\n",
    "labels_train = dataset_train.targets\n",
    "labels_test = dataset_test.targets\n",
    "classes = GalaxyMNISTHighrez.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ad8d2-8d8c-4c37-8b9d-13d4d9a48a93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f4d509-8977-4f8d-a158-8a0bfc75db0b",
   "metadata": {},
   "source": [
    "### Pre-processing \n",
    "The following code is the same as in Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6decf49-9a17-43b2-b659-6a429440326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.pre import pre_processing #  A predefined function to pre-process the data as we did in tutorial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9da92-41d1-41b7-a888-593598261aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing(data, size) function takes two arguments\n",
    "# 1. data: the data to be processed\n",
    "# 2. The size for which the data needs to be reduced.\n",
    "images_trainPre = pre_processing(images_train, 56)\n",
    "images_testPre = pre_processing(images_test, 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d6bae-5885-4ce3-83d7-74dbdaeb8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trainPre.shape # the shape of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba698cf5-b955-43a1-8dcd-e97f4088e2de",
   "metadata": {},
   "source": [
    "Displaying images after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb62536-7817-4f05-9110-263a0f57da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 1\n",
    "columns = 5\n",
    "for j in range(len(GalaxyMNISTHighrez.classes)):\n",
    "    fig = plt.figure(figsize=(8, 8))# Figure is 8 inches by 8 inches\n",
    "    for i in range (columns):    # Create images in each column\n",
    "        train_image = images_trainPre[(labels_train == j)][i]\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(train_image*255,cmap='gray', vmin=0, vmax=255) \n",
    "                            # we have to multiply the image by 255 to restore the original values\n",
    "    print(\"label: \"+str(GalaxyMNISTHighrez.classes[j]))\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444a5c0-dd53-49df-bcd0-fa56e2f874d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cd1b55-87ec-4794-925a-c9bcf919c35f",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "We will try different options for the autoencoder and compare performance. First we'll try a shallow autoencoder with as few layers as possible. Then we'll compare with a deeper autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc59a2f-14d3-47db-90d0-65454e41db44",
   "metadata": {},
   "source": [
    "#### Shallow Autoencoder\n",
    "The autoencoder neural network must have a symmetric structure, and thus must have an even number of layers. We will use a very simple neural network for the autoencoder with just two hidden layers (plus input and output layers). The autoencoder neural network is trained on the data that we pre-processed. The original code can be found [here](https://www.tensorflow.org/tutorials/generative/autoencoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91913044-6f3c-475f-8fc8-c6ce791b5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64 # the number of features to be encoded, this can change \n",
    "num, length, width  = images_trainPre.shape\n",
    "# need to document how excatly it works\n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    # The NN is defined in two parts:encoder and decoder\n",
    "    # Encoder part:\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(), # Input layer-- flattens image into vector\n",
    "      layers.Dense(latent_dim, activation='relu'), # Dense hidden layer\n",
    "    ])\n",
    "    # Decoder part of the NN\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(length*width, activation='sigmoid'), # Dense hidden layer\n",
    "      layers.Reshape((length, width)) # Output layer (reshapes vector back to image size)\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c9344-d9aa-410c-9ffc-aba368e6f963",
   "metadata": {},
   "source": [
    "##### 1) Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ad6c0-dea1-41b3-b06b-d97c495dbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_model = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a8a4d-00b1-4c71-ba94-19e2e3696994",
   "metadata": {},
   "source": [
    "##### 2) Compile model with Adam optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a790d-e822-4931-aa81-a08dd2139d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_model.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360fe70-e8b0-4124-95a6-f82ecf1a65ad",
   "metadata": {},
   "source": [
    "##### 3) Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa553281-1bcc-4644-9431-337f7c6e9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_model.build((None, 56,56,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e35a5-3463-43ec-9104-9146b5a087cb",
   "metadata": {},
   "source": [
    "##### 4) Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4afdbd5",
   "metadata": {},
   "source": [
    "In the training process we use \"early stopping\", which automatically terminates training when there is little or no improvement from epoch to epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74973b4d-eed4-44c5-a4b9-56f64a8098ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14142d42-8a62-4b58-9da4-4ae5cf6eb163",
   "metadata": {},
   "source": [
    "`EarlyStopping()` has a few options:\n",
    "- `monitor (default value 'val_loss')`: Uses validation loss as performance measure to terminate the training.\n",
    "- `patience (default value 0)`: specifies the number of epochs with no improvement. The value 0 means the training is terminated as soon as the performance measure gets worse from one epoch to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbfef81",
   "metadata": {},
   "source": [
    "We're ready to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6b051-ff28-40d8-89da-1707effe9a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shallow_model.fit(np.array(images_trainPre), np.array(images_trainPre),\n",
    "                epochs=50,\n",
    "                shuffle=True,\n",
    "                validation_data=(np.array(images_trainPre), np.array(images_trainPre)), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8d8c7-be81-4aec-a199-f581a8086d59",
   "metadata": {},
   "source": [
    "This code runs very fast, because the model is very shallow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e4e5cc-cac4-47bc-8973-d1c37c4c478d",
   "metadata": {},
   "source": [
    "##### 5) Display the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5329236",
   "metadata": {},
   "source": [
    "Now, Let's compare inputs and outputs, and see if they closely resemble each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ae8e9-3ec3-42e1-bcd1-ccc403c6e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = shallow_model.encoder(images_testPre).numpy()\n",
    "decoded_imgs = shallow_model.decoder(encoded_imgs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae370e2-1298-44f9-9116-4ff3fb40bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the output shape is correct\n",
    "print(decoded_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfc53b-ee82-4e86-95b8-1db0953c618d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display inputs and outputs\n",
    "rows = 1\n",
    "columns = 5\n",
    "for j in range(len(GalaxyMNISTHighrez.classes)):\n",
    "    fig = plt.figure(figsize=(8, 8))# Figure is 8 inches by 8 inches\n",
    "    for i in range (columns):    # Create images in each column\n",
    "        test_image = images_testPre[(labels_test == j)][i]\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(test_image*255,cmap='gray', vmin=0, vmax=255) \n",
    "                            # we have to multiply the image by 255 to restore the original values\n",
    "    print(\"Original: \"+str(GalaxyMNISTHighrez.classes[j]))\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))# Figure is 8 inches by 8 inches\n",
    "    for i in range (columns):    # Create images in each column\n",
    "        test_image = decoded_imgs[(labels_test == j)][i]\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(test_image*255,cmap='gray', vmin=0, vmax=255) \n",
    "                            # we have to multiply the image by 255 to restore the original values\n",
    "    print(\"Reconstructed: \"+str(GalaxyMNISTHighrez.classes[j]))\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc42e49-a2a5-4b60-bb7b-b6b24c716ee8",
   "metadata": {},
   "source": [
    "**Exercise 1:** Which classes do you think will be confused with the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ebe2f-aa22-4283-84d4-1bf2adb00237",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Answer here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf77fc-e573-4e2f-a280-f58a579b41dd",
   "metadata": {},
   "source": [
    "##### 6) Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1a071-4433-4ad4-b4d1-e8f145ed5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_model.save(\"./shallowModel_save\") # saving the model (shallow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d10703-99a6-4cda-956a-90593104d410",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed9483-f19e-468c-a27b-59024b7ca62d",
   "metadata": {},
   "source": [
    "#### Deep convolutional autoencoder\n",
    "A visual comparison of inputs and outputs shows the shallow fully-connected autoencoder does not preserve images very well. So we'll try a more complicated model and compare execution time and image quality. In image classification, convolutional NN's are typically used. So let's try a convolutional NN for our autoencoder. The following autoencoder model is a modified version of the one found [here](https://github.com/ezrafielding/galaxy-cluster/blob/main/autoencoder/galaxyencode.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea26ce-78ce-4870-ab1b-968ac82953b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyEncoder(Model):\n",
    "    def __init__(self):\n",
    "        super(GalaxyEncoder, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential ([\n",
    "            layers.InputLayer(input_shape=(56,56,1)),\n",
    "            layers.Conv2D(16, (3,3), 1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.MaxPool2D((2,2), padding=\"same\", strides=2),\n",
    "            layers.Conv2D(8, (3,3), 1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.MaxPool2D((2,2), padding=\"same\", strides=2),\n",
    "            layers.Flatten()\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential ([\n",
    "            layers.InputLayer(input_shape=(1568)),\n",
    "            layers.Reshape((14, 14, 8)),\n",
    "            layers.UpSampling2D((2,2)),\n",
    "            layers.Conv2DTranspose(8, (3,3), 1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.UpSampling2D((2,2)),\n",
    "            layers.Conv2DTranspose(16, (3,3), 1, padding=\"same\", activation=\"relu\"),\n",
    "            layers.Conv2D(1, (3,3), 1, padding=\"same\", activation=\"sigmoid\")\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c52cf9-7aa9-4252-8610-09682743fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = GalaxyEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a11030",
   "metadata": {},
   "source": [
    "### Exercises ###\n",
    "Following the procedure that was used above for the shallow model, apply the same steps to the deep model that we have just defined. These steps include: 1) define, 2) compiling, 3) building, 4) training, 5) displaying and 6) saving.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e24aef",
   "metadata": {},
   "source": [
    "##### 1) Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4466a-750f-449a-8efa-7a5bb77ee521",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b7d29-edec-490f-a1da-9e5cc5a7438e",
   "metadata": {},
   "source": [
    "##### 2) compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cc1f6-15bc-4494-b78d-228df4abef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911a00c-b5e6-48e6-a757-3b898e89dc55",
   "metadata": {},
   "source": [
    "##### 3) building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358be37-4e83-4adc-a503-1aa4219d7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ffb73-cf0f-46af-82a8-634449909747",
   "metadata": {},
   "source": [
    "##### 4) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b341f1f-cf28-4483-9643-e50ec8cdb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f863b13-dda8-4a09-a4a1-65c397773db5",
   "metadata": {},
   "source": [
    "##### 5) displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131900f-487d-4bb7-bbdb-0fc9bba67c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e2eec-e039-42eb-9ed0-cd1a5c59f029",
   "metadata": {},
   "source": [
    "##### 6) saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d615f-f5ff-4f91-9ea9-77eb13667a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861c799-f263-46b9-b8c4-f0767b65febc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb2e54-2920-417a-89ec-127c94212130",
   "metadata": {},
   "source": [
    "### Extracting the engineered features from the autoencoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a50ad-a58a-421f-8be9-d45dbea2e8b6",
   "metadata": {},
   "source": [
    "For now, will continue just with the shallow model. To proceed, we need to extract the 64 encoded features from the shallow encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3843e6-6456-48f1-8335-986f991d648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96f82d-77de-49de-9be0-5164aba72e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('shallowModel_save/') # recalling the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f5e701",
   "metadata": {},
   "source": [
    "We may apply the encoder to training and testing data to obtained the encoded features for data item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3feec-ac0d-4f5d-b2d8-15a67cfbe68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_features_train = model.encoder.predict(images_trainPre) # extracting the features for the training data\n",
    "auto_features_test = model.encoder.predict(images_testPre)   # extracting the features for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec886b-9488-490a-b6f3-6617a4d4737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df_train = pd.DataFrame(auto_features_train) #turning the data into a dataframe\n",
    "auto_df_test = pd.DataFrame(auto_features_test) #turning the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18753f0e-2865-4fa4-97f0-de025fcba2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auto_df_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33736501-e002-46a3-ac7c-4c0db4c38b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42698a-ea07-4167-8939-d2f3f711f05f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca031aad-2a6d-42a2-bc71-ce97eabd391c",
   "metadata": {},
   "source": [
    "#### **_Saving data for later use_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaddef7-3663-41bb-8f6c-b6d1d26cae85",
   "metadata": {},
   "source": [
    "We can save the data so that we can call it up again in subsequent notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80fb440-3e58-4562-adc3-41e001c18acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store auto_df_train\n",
    "%store auto_df_test\n",
    "%store labels_train\n",
    "%store labels_test\n",
    "%store classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04993e78-0899-48f9-b9b3-abf6603ce9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36b6d3-5e3b-4215-b196-5827c7618b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gal_ker",
   "language": "python",
   "name": "gal_ker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dc42d555f8d6ccc45f2d0db721963f431be3c70e5a851dbe8ed2ba253c6da66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
