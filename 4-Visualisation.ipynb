{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b8b561-a49a-405e-a6d5-9ca11dc1f3b2",
   "metadata": {},
   "source": [
    "# Tutorial 4: Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcb8ac-2e95-4894-9ed4-161dd8277bd6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec1a7d3-339d-4f61-abeb-cc067f21e95d",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62bfbb7-50cf-429c-9bc5-e6de6be53a3e",
   "metadata": {},
   "source": [
    "When dealing with classification data, visualisation is always useful because it can give us an idea of the distribution between the different classes. However, visualization is difficult if the data has many dimensions. To represent the data visually, it is necessary to reduce the dimensionality by identifying the most important features for characterizing the data--this is called _feature reduction_.  There are several methods for doing this. n this tutorial we will show how to use the following methods (for more information about each method, see links):\n",
    "- [Principal component analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html): this is the most widely used method for feature reduction.\n",
    "- [T-distributed Stochastic Neighbor Embedding (tSNE)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE)\n",
    "\n",
    "Additional methods can be found [here](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87860f76-30dc-42b7-b219-76669748a10a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df444414-e204-4d91-910e-f673cc148d71",
   "metadata": {},
   "source": [
    "First let's import some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b758a0ea-6ef3-4571-846e-42d6535455b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e65c7-8ba8-4029-9312-fd3b7cb8dca5",
   "metadata": {},
   "source": [
    "Next let's recall the data. This tutorial will use the extracted features from the shallow autoencoder model (2nd notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5bddbd-6e64-4d4a-953c-b62d2f12bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r auto_df_train \n",
    "%store -r auto_df_test \n",
    "%store -r labels_train\n",
    "%store -r labels_test\n",
    "%store -r classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec896491-c4ee-4d47-ae0c-3166b5defeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 128)\n"
     ]
    }
   ],
   "source": [
    "print(auto_df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e206b42-9063-4ba6-a082-645fb224abcb",
   "metadata": {},
   "source": [
    "In the graphs, we will show two classes at a time to bring out the differences between classes. So we create a list of pairs of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694aac4a-1505-49b1-8f52-13f96f6fb772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smooth_round', 'smooth_cigar', 'edge_on_disk', 'unbarred_spiral']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5f4df4-36cf-47a4-988f-066161f21b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('smooth_round', 'smooth_cigar'), ('smooth_round', 'edge_on_disk'), ('smooth_round', 'unbarred_spiral'), ('smooth_cigar', 'edge_on_disk'), ('smooth_cigar', 'unbarred_spiral'), ('edge_on_disk', 'unbarred_spiral')]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "comb_class = list(combinations(classes, 2))\n",
    "print(comb_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf31a89-8720-4f27-82ca-7544539b5428",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7255076-156f-41fd-b530-cbaf03712f25",
   "metadata": {},
   "source": [
    "### Reduce dimensionality with PCA\n",
    "PCA will take the original data with 64 features and identify two combined features which most effectively characterize the data variation. Each combined feature is a linear combination of the original 64 features (note that combined features may not have a direct practical interpretation--there only use is to characterize the data variation as much as possible).\n",
    "\n",
    "Using `sklearn` it is easy to apply PCA to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec65947f-bad1-45be-be8c-4a31adbae2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804f36e1-a1f7-4cd9-a6f0-e98c7751fda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2)\n"
     ]
    }
   ],
   "source": [
    "pca_sample = PCA(n_components=2).fit_transform(np.array(auto_df_train))\n",
    "print(pca_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23776bf5-bac1-4779-82ed-483aed5f53ce",
   "metadata": {},
   "source": [
    "The shape of the reduced PCA sample is now 8000 x 2 from 8000 x 64, so we can plot these samples in a 2D graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17834260-bc8d-424b-9b12-2ed301299ad8",
   "metadata": {},
   "source": [
    "We will represent the data using a scatterplot, and will superimpose confidence ellipses to bring out the general orientation and extent of the data. A confidence ellipse shows where the data is most heavily concentrated (i.e. where the probability density is highest). Confidence regions are used for predicting new observations with a certain degree of confidence, which depends on the confidence parameter (measured in standard deviations) used to generate the ellipse.\n",
    "\n",
    "The function `draw_Confidence_ellipse` is in the file `ellipse.py` in the `source` subdirectory. The  syntax for the `draw_Confidence_ellipse` command is as follows:\n",
    "\n",
    " `draw_Confidence_ellipse (data1_x, data1_y, data2_x, data2_y, \"y-axis label\", \"x-axis label\", \"title\", x-scale, y-scale)\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37eb2290-a263-4f02-ae31-579892feb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.ellipse import draw_confidence_ellipse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0960c",
   "metadata": {},
   "source": [
    "Now we're ready to create scatterplots for each pair of classes, using the combined features identified by PCA. It's better to plot only a subset of the data--otherwise the scatterplots are too crowded.  So we first define an index variable to select a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e19bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "idx = np.random.choice(1992, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e8b0b",
   "metadata": {},
   "source": [
    "Now for the plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96203583-4c01-47e0-90ce-a48b3abe02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cc in comb_class:\n",
    "    class_i = (np.array(labels_train) == classes.index(cc[0]) ) # getting 1000 random samples from the 1st class\n",
    "    class_j = (np.array(labels_train) == classes.index(cc[1]) ) # getting 1000 random samples from the 2nd class\n",
    "    \n",
    "    draw_confidence_ellipse (pca_sample[:, 0][class_i][idx], pca_sample[:, 1][class_i][idx], \n",
    "                             pca_sample[:, 0][class_j][idx], pca_sample[:, 1][class_j][idx], \n",
    "                             \"comb1\", \"comb2\", cc[0]+\" vs. \"+cc[1], (-10,30), (-7.5,7.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11143e1",
   "metadata": {},
   "source": [
    "Any data samples that fall within both ellipses are likely to be confused between the two classes. Of the above graphs, smooth_round versus smooth_cigar shows the best separation, because many of smooth_cigar points are not in the smooth_round ellipse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce8b05-d74c-440a-bee6-f1333e061974",
   "metadata": {},
   "source": [
    "### Reduce dimensionality with TSNE\n",
    "TSNE is an alternative dimension reduction method developed by Sam Roweis and Geoffrey Hinton. It is nonlinear, while PCA is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332eaade-1e9d-416b-a50f-085102d37db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e984a44-4603-442d-8ab8-2c32379846ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_sample = TSNE(n_components=2, init='pca', perplexity=10, learning_rate = 10).fit_transform(np.array(auto_df_train)[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7803887-0c92-4594-a879-9ecec19e3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cc in comb_class:\n",
    "    class_i = (np.array(labels_train) == classes.index(cc[0]) )[idx] # we can restrict the number of samples from here\n",
    "    class_j = (np.array(labels_train) == classes.index(cc[1]) )[idx]\n",
    "    draw_confidence_ellipse (tsne_sample[:, 0][class_i], tsne_sample[:, 1][class_i], tsne_sample[:, 0][class_j], tsne_sample[:, 1][class_j], \"comb1\", \"comb2\", cc[0]+\" vs \"+cc[1], (-80,80), (-80,80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c992fa-f057-4ec9-893a-cf08e53f395d",
   "metadata": {},
   "source": [
    "The graph from both tools shows that it will be tough to distinguish between the `smooth round` and `unbarred_spiral`, because many of the points coincide. Also, the TSNE method does much better in determining the two classes than the PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073715f0-9596-4fef-afa1-8ca6f742c895",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731c334-06ac-45b8-894a-2dd61beb3f27",
   "metadata": {},
   "source": [
    "**Exercise :** from the List we showed [here](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py)\n",
    "choose another dimension reduction method and show the distibution between pairs of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd3e16-892f-47de-92e8-9bbe130db41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b9504c4-65ba-425f-9872-423065f9ecab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa52da-0a10-4050-95ad-d2b7208e025b",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d217c9-cfc6-42e0-83d8-4e74c5932726",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab3c07-376c-4bdf-a06b-9a062892987c",
   "metadata": {},
   "source": [
    "Note, this tutorial is inspired by this work [here](https://spacetelescope.github.io/hellouniverse/notebooks/hello-universe/Classifying_PanSTARRS_sources_with_unsupervised_learning/Classifying_PanSTARRS_sources_with_unsupervised_learning.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gal_ker",
   "language": "python",
   "name": "gal_ker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
